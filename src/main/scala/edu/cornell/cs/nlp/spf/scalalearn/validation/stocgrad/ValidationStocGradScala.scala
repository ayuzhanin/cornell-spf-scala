package edu.cornell.cs.nlp.spf.scalalearn.validation.stocgrad

import java.util
import java.util.function.Predicate

import edu.cornell.cs.nlp.spf.base.hashvector.HashVectorFactory
import edu.cornell.cs.nlp.spf.ccg.categories.Category
import edu.cornell.cs.nlp.spf.ccg.categories.ICategoryServices
import edu.cornell.cs.nlp.spf.ccg.lexicon.ILexiconImmutable
import edu.cornell.cs.nlp.spf.data.IDataItem
import edu.cornell.cs.nlp.spf.data.ILabeledDataItem
import edu.cornell.cs.nlp.spf.data.collection.IDataCollection
import edu.cornell.cs.nlp.spf.data.utils.IValidator
import edu.cornell.cs.nlp.spf.explat.IResourceRepository
import edu.cornell.cs.nlp.spf.explat.ParameterizedExperiment
import edu.cornell.cs.nlp.spf.explat.resources.IResourceObjectCreator
import edu.cornell.cs.nlp.spf.explat.resources.usage.ResourceUsage
import edu.cornell.cs.nlp.spf.genlex.ccg.ILexiconGenerator
import edu.cornell.cs.nlp.spf.learn.validation.AbstractLearner
import edu.cornell.cs.nlp.spf.learn.validation.perceptron.ValidationPerceptron
import edu.cornell.cs.nlp.spf.learn.validation.stocgrad.ValidationStocGrad
import edu.cornell.cs.nlp.spf.parser.IOutputLogger
import edu.cornell.cs.nlp.spf.parser.IParserOutput
import edu.cornell.cs.nlp.spf.parser.ParsingOp
import edu.cornell.cs.nlp.spf.parser.ccg.model.IDataItemModel
import edu.cornell.cs.nlp.spf.parser.ccg.model.IModelImmutable
import edu.cornell.cs.nlp.spf.parser.ccg.model.Model
import edu.cornell.cs.nlp.spf.parser.filter.IParsingFilterFactory
import edu.cornell.cs.nlp.spf.parser.filter.StubFilterFactory
import edu.cornell.cs.nlp.spf.parser.graph.IGraphParser
import edu.cornell.cs.nlp.spf.parser.graph.IGraphParserOutput
import edu.cornell.cs.nlp.utils.filter.IFilter
import edu.cornell.cs.nlp.utils.log.ILogger
import edu.cornell.cs.nlp.utils.log.LoggerFactory

/**
  * Validation-based stochastic gradient learner.
  * <p>
  * The learner is insensitive to the syntactic category generated by the
  * inference procedure -- only the semantic portion is being validated. However,
  * parsers can be constrained to output only specific syntactic categories, see
  * the parser builders.
  * </p>
  *
  * @author Yoav Artzi
  * @param < SAMPLE>
  *          Data item to use for inference.
  * @param < DI>
  *          Data item for learning.
  * @param < MR>
  *          Meaning representation.
  */
object ValidationStocGradScala {

  class Creator[SAMPLE <: IDataItem[SAMPLE], DI <: ILabeledDataItem[SAMPLE, _], MR](val name: String)
    extends IResourceObjectCreator[ValidationStocGrad[SAMPLE, DI, MR]] {
    def this() = {
      this("learner.validation.stocgrad")
    }

    @SuppressWarnings("unchecked")
    override def create(params: ParameterizedExperiment#Parameters, repo: IResourceRepository): ValidationStocGrad[SAMPLE, DI, MR] = {

      val numIterations =
        if (params.contains("iter")) params.get("iter").toInt
        else 4

      val trainingData: IDataCollection[DI] = repo.get(params.get("data"))

      val trainingDataDebug = new java.util.HashMap[DI, MR]

      val maxSentenceLength = java.lang.Integer.MAX_VALUE

      val lexiconGenerationBeamSize =
        if (params.contains("genlexbeam")) params.get("genlexbeam").toInt
        else 20

      val parser = repo.get(ParameterizedExperiment.PARSER_RESOURCE).asInstanceOf[IGraphParser[SAMPLE, MR]]

      val parserOutputLogger: IOutputLogger[MR] =
        if (params.contains("parseLogger")) repo.get(params.get("parseLogger")).asInstanceOf[IOutputLogger[MR]]
        else { (_: IParserOutput[MR], _: IDataItemModel[MR], _: String) => () }

      val alpha0 =
        if (params.contains("alpha0")) params.get("alpha0").toDouble
        else 1.0

      val c =
        if (params.contains("c")) params.get("c").toDouble
        else 0.0001

      val validator = repo.get(params.get("validator")).asInstanceOf[IValidator[DI, MR]]

      val conflateGenlexAndPrunedParses =
        if (params.contains("conflateParses")) params.get("conflateParses").toBoolean
        else false

      val errorDriven =
        if (params.contains("errorDriven")) params.get("errorDriven").toBoolean
        else false

      val (genlex, categoryServices) =
        if (params.contains("genlex"))
          (repo.get(params.get("genlex")).asInstanceOf[ILexiconGenerator[DI, MR, IModelImmutable[SAMPLE, MR]]],
            repo.get(ParameterizedExperiment.CATEGORY_SERVICES_RESOURCE).asInstanceOf[ICategoryServices[MR]])
        else (null, null)

      val filter: IFilter[DI] =
        if (params.contains("filter")) repo.get(params.get("filter")).asInstanceOf[IFilter[DI]]
        else { (_: DI) => true }

      val parsingFilterFactory =
        if (params.contains("filterFactory")) repo.get(params.get("filterFactory")).asInstanceOf[IParsingFilterFactory[DI, MR]]
        else new StubFilterFactory[DI, MR]

      new ValidationStocGrad[SAMPLE, DI, MR](
        numIterations,
        trainingData,
        trainingDataDebug,
        maxSentenceLength,
        lexiconGenerationBeamSize,
        parser,
        parserOutputLogger,
        alpha0,
        c,
        validator,
        conflateGenlexAndPrunedParses,
        errorDriven,
        categoryServices,
        genlex,
        filter,
        parsingFilterFactory)
    }

    override def `type`: String = name

    override def usage: ResourceUsage =
      new ResourceUsage.Builder(`type`, classOf[ValidationPerceptron[_ <: IDataItem[_], _ <: ILabeledDataItem[_, _], _]])
        .setDescription("Validation-based stochastic gradient learner")
        .addParam("data", "id", "Training data")
        .addParam("genlex", "ILexiconGenerator", "GENLEX procedure")
        .addParam("filterFactory", classOf[IParsingFilterFactory[_, _]], "Factory to create parsing filters (optional).")
        .addParam("conflateParses", "boolean", "Recyle lexical induction parsing output as pruned parsing output")
        .addParam("parseLogger", "id", "Parse logger for debug detailed logging of parses")
        .addParam("genlexbeam", "int", "Beam to use for GENLEX inference (parsing).")
        .addParam("filter", "IFilter", "Processing filter")
        .addParam("iter", "int", "Number of training iterations")
        .addParam("validator", "IValidator", "Validation function")
        .addParam("tester", "ITester", "Intermediate tester to use between epochs")
        .addParam("c", "double", "Learing rate c parameter, temperature=alpha_0/(1+c*tot_number_of_training_instances)")
        .addParam("alpha0", "double", "Learing rate alpha0 parameter, temperature=alpha_0/(1+c*tot_number_of_training_instances)")
        .addParam("errorDriven", "boolean", "Error driven lexical generation, if the can generate a valid parse, skip lexical induction")
        .build
  }

}

class ValidationStocGradScala[SAMPLE <: IDataItem[SAMPLE], DI <: ILabeledDataItem[SAMPLE, _], MR] private
          (val numIterations: Int,
           val trainingData: IDataCollection[DI],
           val trainingDataDebug: util.Map[DI, MR],
           val maxSentenceLength: Int,
           val lexiconGenerationBeamSize: Int,
           val parser: IGraphParser[SAMPLE, MR],
           val parserOutputLogger: IOutputLogger[MR],
           val alpha0: Double,
           val c: Double,
           val validator: IValidator[DI, MR],
           val conflateGenlexAndPrunedParses: Boolean,
           val errorDriven: Boolean,
           val categoryServices: ICategoryServices[MR],
           val genlex: ILexiconGenerator[DI, MR, IModelImmutable[SAMPLE, MR]],
           val processingFilter: IFilter[DI],
           val parsingFilterFactory: IParsingFilterFactory[DI, MR])
    extends AbstractLearner[SAMPLE, DI, IGraphParserOutput[MR], MR](
                numIterations,
                trainingData,
                trainingDataDebug,
                lexiconGenerationBeamSize,
                parserOutputLogger,
                conflateGenlexAndPrunedParses,
                errorDriven,
                categoryServices,
                genlex,
                processingFilter,
                parsingFilterFactory) {

  import AbstractLearner._

  val log: ILogger = LoggerFactory.create(classOf[ValidationStocGrad[SAMPLE, DI, MR]])

  log.info(s"Init ValidationStocGrad: numIterations=$numIterations, trainingData.size()=${trainingData.size}, trainingDataDebug.size()=${trainingDataDebug.size}, maxSentenceLength=$maxSentenceLength ...")
  log.info(s"Init ValidationStocGrad: ... lexiconGenerationBeamSize=$lexiconGenerationBeamSize")
  log.info(s"Init ValidationStocGrad: ... conflateParses=${if (conflateGenlexAndPrunedParses) "true" else "false"}, erroDriven=${if (errorDriven) "true" else "false"}")
  log.info(s"Init ValidationStocGrad: ... c=$c, alpha0=$alpha0")
  log.info(s"Init ValidationStocGrad: ... parsingFilterFactory=$parsingFilterFactory")

  private var stocGradientNumUpdates = 0

  override def train(model: Model[SAMPLE, MR]): Unit = {
    stocGradientNumUpdates = 0
    super.train(model)
  }

  override protected def parameterUpdate(dataItem: DI,
                                         realOutput: IGraphParserOutput[MR],
                                         goodOutput: IGraphParserOutput[MR],
                                         model: Model[SAMPLE, MR],
                                         itemCounter: Int,
                                         epochNumber: Int): Unit = {
    // Create the update
    val update = HashVectorFactory.create

    // Step A: Compute the positive half of the update: conditioned on getting successful validation
    val filter: IFilter[Category[MR]] = (e: Category[MR]) => validate(dataItem, e.getSemantics)
    val logConditionedNorm = goodOutput.logNorm(filter)
    // No positive update, skip the update.

    if (logConditionedNorm == java.lang.Double.NEGATIVE_INFINITY) {
      log.info("No positive update")
      return
    }
    else {
      // Case have complete valid parses.
      val expectedFeatures = goodOutput.logExpectedFeatures(filter)
      expectedFeatures.add(-logConditionedNorm)
      expectedFeatures.applyFunction((value: Double) => Math.exp(value))
      expectedFeatures.dropNoise()
      expectedFeatures.addTimesInto(1.0, update)

      // Record if the output LF equals the available gold LF (if one is available), otherwise, record using validation signal.
      stats.count("Valid", epochNumber)
      if (realOutput.getBestDerivations.size == 1 && isGoldDebugCorrect(dataItem, realOutput.getBestDerivations.get(0).getSemantics)) stats.appendSampleStat(itemCounter, epochNumber, GOLD_LF_IS_MAX)
      // Record if a valid parse was found.
      else stats.appendSampleStat(itemCounter, epochNumber, HAS_VALID_LF)

      log.info(s"Positive update: $expectedFeatures")
    }

    // Step B: Compute the negative half of the update: expectation under the current model
    val logNorm = realOutput.logNorm
    if (logNorm == java.lang.Double.NEGATIVE_INFINITY) log.info("No negative update.")
    else {
      // Case have complete parses.
      val expectedFeatures = realOutput.logExpectedFeatures
      expectedFeatures.add(-logNorm)
      expectedFeatures.applyFunction((value: Double) => Math.exp(value))
      expectedFeatures.dropNoise()
      expectedFeatures.addTimesInto(-1.0, update)
      log.info(s"Negative update: $expectedFeatures")
    }

    // Step C: Apply the update. Validate the update
    if (!model.isValidWeightVector(update)) throw new IllegalStateException(s"invalid update: $update" )
    // Scale the update
    val scale = alpha0 / (1.0 + c * stocGradientNumUpdates)
    update.multiplyBy(scale)
    update.dropNoise()
    stocGradientNumUpdates += 1
    log.info(s"Scale: $scale")
    if (update.size == 0) {
      log.info("No update")
      return
    }
    else log.info(s"Update: $update")

    // Check for NaNs and super large updates
    if (update.isBad) {
      log.error(s"Bad update: $update -- log-norm: $logNorm -- features: ${model.getTheta.printValues(update)}")
      throw new IllegalStateException("bad update")
    }
    else {
      if (!update.valuesInRange(-100, 100))
        log.error(s"Large update: $update -- log-norm: $logNorm -- features: ${model.getTheta.printValues(update)}")

      // Do the update
      update.addTimesInto(1, model.getTheta)
      stats.appendSampleStat(itemCounter, epochNumber, TRIGGERED_UPDATE)
    }
  }

  override protected def parse(dataItem: DI, dataItemModel: IDataItemModel[MR]): IGraphParserOutput[MR] =
    parser.parse(dataItem.getSample, dataItemModel)

  override protected def parse(dataItem: DI,
                               pruningFilter: Predicate[ParsingOp[MR]],
                               dataItemModel: IDataItemModel[MR]): IGraphParserOutput[MR] =
    parser.parse(dataItem.getSample, pruningFilter, dataItemModel)

  override protected def parse(dataItem: DI,
                               pruningFilter: Predicate[ParsingOp[MR]],
                               dataItemModel: IDataItemModel[MR],
                               generatedLexicon: ILexiconImmutable[MR],
                               beamSize: Integer): IGraphParserOutput[MR] =
    parser.parse(dataItem.getSample, pruningFilter, dataItemModel, false, generatedLexicon, beamSize)

  override protected def validate(dataItem: DI, hypothesis: MR): Boolean =
    validator.isValid(dataItem, hypothesis)
}
