/** *****************************************************************************
  * Copyright (C) 2011 - 2015 Yoav Artzi, All rights reserved.
  * <p>
  * This program is free software; you can redistribute it and/or modify it under
  * the terms of the GNU General Public License as published by the Free Software
  * Foundation; either version 2 of the License, or any later version.
  * <p>
  * This program is distributed in the hope that it will be useful, but WITHOUT
  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
  * FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
  * details.
  * <p>
  * You should have received a copy of the GNU General Public License along with
  * this program; if not, write to the Free Software Foundation, Inc., 51
  * Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
  * ******************************************************************************/
package edu.cornell.cs.nlp.spf.learn.validation.perceptron

import java.util
import java.util.{HashMap, HashSet, LinkedList, List, Map, Set}
import java.util.function.Predicate

import edu.cornell.cs.nlp.spf.base.hashvector.HashVectorFactory
import edu.cornell.cs.nlp.spf.base.hashvector.IHashVector
import edu.cornell.cs.nlp.spf.ccg.categories.ICategoryServices
import edu.cornell.cs.nlp.spf.ccg.lexicon.ILexiconImmutable
import edu.cornell.cs.nlp.spf.data.IDataItem
import edu.cornell.cs.nlp.spf.data.ILabeledDataItem
import edu.cornell.cs.nlp.spf.data.collection.IDataCollection
import edu.cornell.cs.nlp.spf.data.utils.IValidator
import edu.cornell.cs.nlp.spf.explat.IResourceRepository
import edu.cornell.cs.nlp.spf.explat.ParameterizedExperiment
import edu.cornell.cs.nlp.spf.explat.ParameterizedExperiment.Parameters
import edu.cornell.cs.nlp.spf.explat.resources.IResourceObjectCreator
import edu.cornell.cs.nlp.spf.explat.resources.usage.ResourceUsage
import edu.cornell.cs.nlp.spf.genlex.ccg.ILexiconGenerator
import edu.cornell.cs.nlp.spf.learn.validation.AbstractLearner
import edu.cornell.cs.nlp.spf.parser.IDerivation
import edu.cornell.cs.nlp.spf.parser.IOutputLogger
import edu.cornell.cs.nlp.spf.parser.IParser
import edu.cornell.cs.nlp.spf.parser.IParserOutput
import edu.cornell.cs.nlp.spf.parser.ParsingOp
import edu.cornell.cs.nlp.spf.parser.ccg.model.IDataItemModel
import edu.cornell.cs.nlp.spf.parser.ccg.model.IModelImmutable
import edu.cornell.cs.nlp.spf.parser.ccg.model.Model
import edu.cornell.cs.nlp.spf.parser.filter.IParsingFilterFactory
import edu.cornell.cs.nlp.spf.parser.filter.StubFilterFactory
import edu.cornell.cs.nlp.utils.composites.Pair
import edu.cornell.cs.nlp.utils.filter.IFilter
import edu.cornell.cs.nlp.utils.log.ILogger
import edu.cornell.cs.nlp.utils.log.LoggerFactory

import scala.collection.JavaConverters._

/**
  * Validation-based perceptron learner. See Artzi and Zettlemoyer 2013 for
  * detailed description.
  * <p>
  * The learner is insensitive to the syntactic category generated by the
  * inference procedure -- only the semantic portion is being validated. However,
  * parsers can be constrained to output only specific syntactic categories, see
  * the parser builders.
  * </p>
  * <p>
  * Parameter update step inspired by: Natasha Singh-Miller and Michael Collins.
  * 2007. Trigger-based Language Modeling using a Loss-sensitive Perceptron
  * Algorithm. In proceedings of ICASSP 2007.
  * </p>
  *
  * @author Yoav Artzi
  * @param < SAMPLE>
  *          Data item to use for inference.
  * @param < DI>
  *          Data item for learning.
  * @param < MR>
  *          Meaning representation.
  */
object ValidationPerceptronImpl {

  private def constructUpdate[MR, P <: IDerivation[MR], MODEL <: IModelImmutable[(_$1) forSome {type _$1}, MR]](violatingValidParses: util.List[P], violatingInvalidParses: util.List[P], model: MODEL) = { // Create the parameter update
    val update = HashVectorFactory.create
    // Get the update for valid violating samples
    import scala.collection.JavaConversions._
    for (parse <- violatingValidParses) {
      parse.getAverageMaxFeatureVector.addTimesInto(1.0 / violatingValidParses.size, update)
    }
    // Get the update for the invalid violating samples
    import scala.collection.JavaConversions._
    for (parse <- violatingInvalidParses) {
      parse.getAverageMaxFeatureVector.addTimesInto(-1.0 * (1.0 / violatingInvalidParses.size), update)
    }
    // Prune small entries from the update
    update.dropNoise()
    // Validate the update
    if (!model.isValidWeightVector(update)) throw new IllegalStateException("invalid update: " + update)
    update
  }

  private def marginViolatingSets[LF, P <: IDerivation[LF], MODEL <: IModelImmutable[(_$1) forSome {type _$1}, LF]](model: MODEL, margin: Double, validParses: util.List[P], invalidParses: util.List[P]) = { // Construct margin violating sets
    val violatingValidParses = new util.LinkedList[P]
    val violatingInvalidParses = new util.LinkedList[P]
    // Flags to mark that we inserted a parse into the violating
    // sets, so no need to check for its violation against others
    val validParsesFlags = new Array[Boolean](validParses.size)
    val invalidParsesFlags = new Array[Boolean](invalidParses.size)
    var validParsesCounter = 0
    import scala.collection.JavaConversions._
    for (validParse <- validParses) {
      var invalidParsesCounter = 0
      import scala.collection.JavaConversions._
      for (invalidParse <- invalidParses) {
        if (!validParsesFlags(validParsesCounter) || !invalidParsesFlags(invalidParsesCounter)) { // Create the delta vector if needed, we do it only
          // once. This is why we check if we are going to
          // need it in the above 'if'.
          val featureDelta = validParse.getAverageMaxFeatureVector.addTimes(-1.0, invalidParse.getAverageMaxFeatureVector)
          val deltaScore = model.score(featureDelta)
          // Test valid parse for insertion into violating
          // valid parses
          if (!validParsesFlags(validParsesCounter)) { // Case this valid sample is still not in the
            // violating set
            if (deltaScore < margin * featureDelta.l1Norm) { // Case of violation
              // Add to the violating set
              violatingValidParses.add(validParse)
              // Mark flag, so we won't test it again
              validParsesFlags(validParsesCounter) = true
            }
          }
          // Test invalid parse for insertion into
          // violating invalid parses
          if (!invalidParsesFlags(invalidParsesCounter)) { // Case this invalid sample is still not in
            // the violating set
            if (deltaScore < margin * featureDelta.l1Norm) {
              violatingInvalidParses.add(invalidParse)
              invalidParsesFlags(invalidParsesCounter) = true
            }
          }
        }
        // Increase the counter, as we move to the next sample
        invalidParsesCounter += 1
      }
      validParsesCounter += 1
    }
    Pair.of(violatingValidParses, violatingInvalidParses)
  }

  /**
    * Builder for {@link ValidationPerceptron}.
    *
    * @author Yoav Artzi
    */
  class Builder[SAMPLE <: IDataItem[(_$1) forSome {type _$1}], DI <: ILabeledDataItem[SAMPLE, (_$1) forSome {type _$1}], MR](/** Training data */
                                                                                                                             val trainingData: IDataCollection[DI], val parser: IParser[SAMPLE, MR], val validator: IValidator[DI, MR]) {
    /**
      * Required for lexicon learning.
      */
    private var categoryServices = null
    /**
      * Recycle the lexical induction parser output as the pruned one for
      * parameter update.
      */
    private var conflateGenlexAndPrunedParses = false
    private var errorDriven = false
    /**
      * GENLEX procedure. If 'null' skips lexicon induction.
      */
    private var genlex = null
    /**
      * Use hard updates. Meaning: consider only highest-scored valid parses
      * for parameter updates, instead of all valid parses.
      */
    private var hardUpdates = false
    /**
      * Beam size to use when doing loss sensitive pruning with generated
      * lexicon.
      */
    private var lexiconGenerationBeamSize = 20
    /** Margin to scale the relative loss function */
    private var margin = 1.0
    /** Number of training iterations */
    private var numIterations = 4
    private var parserOutputLogger = new IOutputLogger[MR]() {
      override def log(output: IParserOutput[MR], dataItemModel: IDataItemModel[MR], tag: String): Unit = {
        // Stub.
      }
    }
    private var parsingFilterFactory = new StubFilterFactory[DI, MR]
    /**
      * Processing filter, if 'false', skip sample.
      */
    private var processingFilter = (e: DI) => true
    /**
      * Mapping a subset of training samples into their gold label for debug.
      */
    private var trainingDataDebug = new util.HashMap[DI, MR]

    def build = new ValidationPerceptron[SAMPLE, DI, MR](numIterations, trainingData, trainingDataDebug, lexiconGenerationBeamSize, parser, parserOutputLogger, conflateGenlexAndPrunedParses, errorDriven, categoryServices, genlex, margin, hardUpdates, validator, processingFilter, parsingFilterFactory)

    def setConflateGenlexAndPrunedParses(conflateGenlexAndPrunedParses: Boolean): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.conflateGenlexAndPrunedParses = conflateGenlexAndPrunedParses
      this
    }

    def setErrorDriven(errorDriven: Boolean): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.errorDriven = errorDriven
      this
    }

    def setGenlex(genlex: ILexiconGenerator[DI, MR, IModelImmutable[SAMPLE, MR]], categoryServices: ICategoryServices[MR]): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.genlex = genlex
      this.categoryServices = categoryServices
      this
    }

    def setHardUpdates(hardUpdates: Boolean): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.hardUpdates = hardUpdates
      this
    }

    def setLexiconGenerationBeamSize(lexiconGenerationBeamSize: Int): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.lexiconGenerationBeamSize = lexiconGenerationBeamSize
      this
    }

    def setMargin(margin: Double): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.margin = margin
      this
    }

    def setNumTrainingIterations(numTrainingIterations: Int): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.numIterations = numTrainingIterations
      this
    }

    def setParserOutputLogger(parserOutputLogger: IOutputLogger[MR]): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.parserOutputLogger = parserOutputLogger
      this
    }

    def setParsingFilterFactory(parsingFilterFactory: IParsingFilterFactory[DI, MR]): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.parsingFilterFactory = parsingFilterFactory
      this
    }

    def setProcessingFilter(processingFilter: IFilter[DI]): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.processingFilter = processingFilter
      this
    }

    def setTrainingDataDebug(trainingDataDebug: util.Map[DI, MR]): ValidationPerceptron.Builder[SAMPLE, DI, MR] = {
      this.trainingDataDebug = trainingDataDebug
      this
    }
  }

  class Creator[SAMPLE <: IDataItem[(_$1) forSome {type _$1}], DI <: ILabeledDataItem[SAMPLE, (_$1) forSome {type _$1}], MR](val `type`: String) extends IResourceObjectCreator[ValidationPerceptron[SAMPLE, DI, MR]] {
    def this {
      this("learner.validation.perceptron")
    }

    @SuppressWarnings(Array("unchecked")) override def create(params: ParameterizedExperiment#Parameters, repo: IResourceRepository): ValidationPerceptron[SAMPLE, DI, MR] = {
      val trainingData = repo.get(params.get("data"))
      val builder = new ValidationPerceptron.Builder[SAMPLE, DI, MR](trainingData, repo.get(ParameterizedExperiment.PARSER_RESOURCE).asInstanceOf[IParser[SAMPLE, MR]], repo.get(params.get("validator")).asInstanceOf[IValidator[DI, MR]])
      if ("true" == params.get("hard")) builder.setHardUpdates(true)
      if (params.contains("parseLogger")) builder.setParserOutputLogger(repo.get(params.get("parseLogger")).asInstanceOf[IOutputLogger[MR]])
      if (params.contains("genlex")) builder.setGenlex(repo.get(params.get("genlex")).asInstanceOf[ILexiconGenerator[DI, MR, IModelImmutable[SAMPLE, MR]]], repo.get(ParameterizedExperiment.CATEGORY_SERVICES_RESOURCE).asInstanceOf[ICategoryServices[MR]])
      if (params.contains("genlexbeam")) builder.setLexiconGenerationBeamSize(Integer.valueOf(params.get("genlexbeam")))
      if (params.contains("conflateParses")) builder.setConflateGenlexAndPrunedParses("true" == params.get("conflateParses"))
      if (params.contains("errorDriven")) builder.setErrorDriven("true" == params.get("errorDriven"))
      if (params.contains("margin")) builder.setMargin(Double.valueOf(params.get("margin")))
      if (params.contains("filterFactory")) builder.setParsingFilterFactory(repo.get(params.get("filterFactory")).asInstanceOf[IParsingFilterFactory[DI, MR]])
      if (params.contains("filter")) builder.setProcessingFilter(repo.get(params.get("filter")).asInstanceOf[IFilter[DI]])
      if (params.contains("iter")) builder.setNumTrainingIterations(Integer.valueOf(params.get("iter")))
      builder.build
    }

    override def `type`: String = `type`

    override def usage: ResourceUsage = new ResourceUsage.Builder(`type`, classOf[ValidationPerceptron[_ <: IDataItem[_], _ <: ILabeledDataItem[_, _], _]]).setDescription("Validation-based perceptron").addParam("data", "id", "Training data").addParam("genlex", "ILexiconGenerator", "GENLEX procedure").addParam("filterFactory", classOf[IParsingFilterFactory[_, _]], "Factory to create parsing filters (optional).").addParam("hard", "boolean", "Use hard updates (i.e., only use max scoring valid parses/evaluation as positive samples). Options: true, false. Default: false").addParam("parseLogger", "id", "Parse logger for debug detailed logging of parses").addParam("tester", "ITester", "Intermediate tester to use between epochs").addParam("genlexbeam", "int", "Beam to use for GENLEX inference (parsing).").addParam("margin", "double", "Margin to use for updates. Updates will be done when this margin is violated.").addParam("filter", "IFilter", "Processing filter").addParam("iter", "int", "Number of training iterations").addParam("validator", "IValidator", "Validation function").addParam("conflateParses", "boolean", "Recyle lexical induction parsing output as pruned parsing output").addParam("errorDriven", "boolean", "Error driven lexical generation, if the can generate a valid parse, skip lexical induction").build
  }

}

class ValidationPerceptronImpl[SAMPLE <: IDataItem[_],
                              DI <: ILabeledDataItem[SAMPLE, _],
                              MR] private(val numIterations: Int,
                                          val trainingData: IDataCollection[DI],
                                          val trainingDataDebug: java.util.Map[DI, MR],
                                          val lexiconGenerationBeamSize: Int,
                                          val parser: IParser[SAMPLE, MR],
                                          val parserOutputLogger: IOutputLogger[MR],
                                          val conflateGenlexAndPrunedParses: Boolean,
                                          val errorDriven: Boolean,
                                          val categoryServices: ICategoryServices[MR],
                                          val genlex: ILexiconGenerator[DI, MR, IModelImmutable[SAMPLE, MR]],
                                          val margin: Double,
                                          val hardUpdates: Boolean,
                                          val validator: IValidator[DI, MR],
                                          val processingFilter: IFilter[DI],
                                          val parsingFilterFactory: IParsingFilterFactory[DI, MR])
  extends AbstractLearner[SAMPLE, DI, IParserOutput[MR], MR](numIterations,
                                                            trainingData,
                                                            trainingDataDebug,
                                                            lexiconGenerationBeamSize,
                                                            parserOutputLogger,
                                                            conflateGenlexAndPrunedParses,
                                                            errorDriven,
                                                            categoryServices,
                                                            genlex,
                                                            processingFilter,
                                                            parsingFilterFactory) {

  val log: ILogger = LoggerFactory.create(classOf[ValidationPerceptron[_ <: IDataItem[_], _ <: ILabeledDataItem[_, _], _]])

  log.info(s"Init ValidationStocGrad: numIterations=$numIterations, margin=$margin, trainingData.size()=${trainingData.size}, trainingDataDebug.size()=${trainingDataDebug.size}  ...")
  log.info(s"Init ValidationStocGrad: ... lexiconGenerationBeamSize=$lexiconGenerationBeamSize")
  log.info(s"Init ValidationStocGrad: ... conflateParses=${if (conflateGenlexAndPrunedParses) "true" else "false"}, errorDriven=${if (errorDriven) "true" else "false"}")
  log.info(s"Init ValidationStocGrad: ... parsingFilterFactory=$parsingFilterFactory")

  /**
    * Collect valid and invalid parses.
    *
    * @param dataItem
    * @param realOutput
    * @param goodOutput
    * @return
    */
  private def createValidInvalidSets(dataItem: DI, realOutput: IParserOutput[MR], goodOutput: IParserOutput[MR]) = {
    // Collect invalid parses from readlOutput
    val invalidParses: scala.List[IDerivation[MR]] = realOutput.getAllDerivations.asScala.filter(parse => !validate(dataItem, parse.getSemantics)).toList

    // Track invalid parses, so we won't aggregate a parse more than once this is an approximation, but it's a best effort
    val invalidSemantics = invalidParses.toSet

    // Collect valid and invalid parses from goodOutput
    val starting = (-java.lang.Double.MAX_VALUE, scala.List.empty[IDerivation[MR]], invalidParses)
    val (_, valids, invalids) = goodOutput.getAllDerivations.asScala.foldLeft(starting){ (accumulated, parse) =>
      val (validScore, valids, invalids) = accumulated
      if (validate(dataItem, parse.getSemantics))
        if (hardUpdates)
          if (parse.getScore > validScore) (parse.getScore, List(parse), invalids)
          else if (parse.getScore == validScore) (validScore, valids :+ parse, invalids)
          else accumulated
        else (validScore, valids :+ parse, invalids)
      else if (!invalidSemantics.contains(parse)) (validScore, valids, invalids :+ parse)
      else accumulated
    }

    (valids, invalids)
  }

  override protected def parameterUpdate(dataItem: DI,
                                         realOutput: IParserOutput[MR],
                                         goodOutput: IParserOutput[MR],
                                         model: Model[SAMPLE, MR],
                                         itemCounter: Int,
                                         epochNumber: Int): Unit = {
    val dataItemModel = model.createDataItemModel(dataItem.getSample)

    // Split all parses to valid and invalid sets
    val (validParses, invalidParses) = createValidInvalidSets(dataItem, realOutput, goodOutput)

    log.info(s"${validParses.size} valid parses, ${invalidParses.size} invalid parses")
    log.info("Valid parses:")

    validParses.foreach(logParse(dataItem, _, true, true, dataItemModel))

    // Record if the output LF equals the available gold LF (if one is available), otherwise, record using validation signal.
    if (realOutput.getBestDerivations.size == 1 && isGoldDebugCorrect(dataItem, realOutput.getBestDerivations.get(0).getSemantics))
      stats.appendSampleStat(itemCounter, epochNumber, GOLD_LF_IS_MAX)
    else if (validParses.nonEmpty)
      // Record if a valid parse was found.
      stats.appendSampleStat(itemCounter, epochNumber, HAS_VALID_LF)

    if (validParses.nonEmpty) stats.count("Valid", epochNumber)

    // Skip update if there are no valid or invalid parses
    if (validParses.isEmpty || invalidParses.isEmpty) {
      log.info("No valid/invalid parses -- skipping")
      return
    }

    val (violatingValidParses, violatingInvalidParses) = marginViolatingSets(model, margin, validParses, invalidParses)
    ValidationPerceptron.LOG.info("%d violating valid parses, %d violating invalid parses", violatingValidParses.size, violatingInvalidParses.size)
    if (violatingValidParses.isEmpty) {
      log.info("There are no violating valid/invalid parses -- skipping")
      return
    }

    log.info("Violating valid parses: ")
    violatingValidParses.foreach(logParse(dataItem, _, true, true, dataItemModel))

    log.info("Violating invalid parses: ")
    violatingInvalidParses.foreach(logParse(dataItem, _, false, true, dataItemModel))

    // Construct weight update vector
    val update = constructUpdate(violatingValidParses, violatingInvalidParses, model)
    // Update the parameters vector
    log.info("Update: %s", update)
    update.addTimesInto(1.0, model.getTheta)
    stats.appendSampleStat(itemCounter, epochNumber, TRIGGERED_UPDATE)
  }

  override protected def parse(dataItem: DI, dataItemModel: IDataItemModel[MR]): IParserOutput[MR] = parser.parse(dataItem.getSample, dataItemModel)

  override protected def parse(dataItem: DI, pruningFilter: Predicate[ParsingOp[MR]], dataItemModel: IDataItemModel[MR]): IParserOutput[MR] = parser.parse(dataItem.getSample, pruningFilter, dataItemModel)

  override protected def parse(dataItem: DI, pruningFilter: Predicate[ParsingOp[MR]], dataItemModel: IDataItemModel[MR], generatedLexicon: ILexiconImmutable[MR], beamSize: Integer): IParserOutput[MR] = parser.parse(dataItem.getSample, pruningFilter, dataItemModel, false, generatedLexicon, beamSize)

  override protected def validate(dataItem: DI, hypothesis: MR): Boolean = validator.isValid(dataItem, hypothesis)
}
